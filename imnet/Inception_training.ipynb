{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "SwiQQCjdvnqJ",
    "outputId": "561e24eb-ebad-4238-94cd-52f70c5ebe7b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f04af2c8ce59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# how to access GDrive https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"drive\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"My Drive\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "\n",
    "# how to access GDrive https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\n",
    "from google.colab import files, drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "gdir = os.path.join(os.getcwd(), \"drive\", \"My Drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoKegeKl28VU"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MojfFLIl2-En",
    "outputId": "9f484d10-bdfc-4b31-cb70-c23bd3a60eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Torch Dataset and IMNet Loading\n",
    "import torch\n",
    "from xml.etree import ElementTree\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw\n",
    "from  collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# Model development and training\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Filesystem and parallelization\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Utility \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Constants for parallelization\n",
    "_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# from ImageNet\n",
    "synsets = {\n",
    "    \"judas\": \"n12513613\",\n",
    "    \"palm\": \"n12582231\",\n",
    "    \"pine\": \"n11608250\",\n",
    "    \"china tree\": \"n12741792\",\n",
    "    \"fig\": \"n12401684\",\n",
    "    \"cabbage\": \"n12478768\",\n",
    "    \"cacao\": \"n12201580\",\n",
    "    \"kapok\": \"n12190410\",\n",
    "    \"iron\": \"n12317296\",\n",
    "    \"linden\": \"n12202936\",\n",
    "    \"pepper\": \"n12765115\",\n",
    "    \"rain\": \"n11759853\",\n",
    "    \"dita\": \"n11770256\",\n",
    "    \"alder\": \"n12284262\",\n",
    "    \"silk\": \"n11759404\",\n",
    "    \"coral\": \"n12527738\",\n",
    "    \"huisache\": \"n11757851\",\n",
    "    \"fringe\": \"n12302071\",\n",
    "    \"dogwood\": \"n12946849\",\n",
    "    \"cork\": \"n12713866\",\n",
    "    \"ginkgo\": \"n11664418\",\n",
    "    \"golden shower\": \"n12492106\",\n",
    "    \"balata\": \"n12774299\",\n",
    "    \"baobab\": \"n12189987\",\n",
    "    \"sorrel\": \"n12242409\",\n",
    "    \"Japanese pagoda\": \"n12570394\",\n",
    "    \"Kentucky coffee\": \"n12496427\",\n",
    "    \"Logwood\": \"n12496949\"\n",
    "}\n",
    "\n",
    "\n",
    "print (\"Device:\" , _DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wshbg0pg3IHX"
   },
   "source": [
    "\n",
    "## Dataset Creation\n",
    "Define datasets for ImageNet and Greenstand sources. Greenstand species classes are yet unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJ2MNziQ3B-B"
   },
   "outputs": [],
   "source": [
    "class ImnetDataset(data.Dataset):\n",
    "    \n",
    "    # initialise function of class\n",
    "    def __init__(self, dir, synsets, transforms=None, device=None, one_hot=False):\n",
    "        # the data directories\n",
    "        self.img_dir = os.path.join(dir, \"original_images\")\n",
    "        self.bb_dir = os.path.join(dir, \"bounding_boxes\")\n",
    "\n",
    "        #synsets library to get the associated class\n",
    "        self.synsets = synsets\n",
    "        self.rev_synsets = {y:x for x,y in zip(synsets.keys(), synsets.values())}\n",
    "        self.classes = list(self.synsets.keys())\n",
    "        # self.target = target\n",
    "\n",
    "        self.one_hot = one_hot\n",
    "        self.imgs = []\n",
    "\n",
    "        for i in self.classes:\n",
    "          temp_imgs = list(sorted(os.listdir(os.path.join(self.img_dir, i))))\n",
    "          for img_path in temp_imgs:\n",
    "            #in every directory the \"tar\" file is still present\n",
    "            if not \"tar\" in img_path:\n",
    "              name = img_path.split('.')[0]\n",
    "              self.imgs.append(name)\n",
    "\n",
    "        self.bb_dict = {}\n",
    "        for f, _, d in os.walk(self.bb_dir):\n",
    "          for file in d:\n",
    "            if os.path.splitext(file)[1] == \".xml\":\n",
    "              tree = ElementTree.parse(os.path.join(f, file))\n",
    "              root = tree.getroot()\n",
    "              obj = root.find(\"object\")\n",
    "              b = obj.find(\"bndbox\")\n",
    "              xmin = int(b.find(\"xmin\").text)\n",
    "              ymin = int(b.find(\"ymin\").text)\n",
    "              xmax = int(b.find(\"xmax\").text)\n",
    "              ymax = int(b.find(\"ymax\").text)\n",
    "              self.bb_dict[os.path.join(f, file)] =  (xmin, ymin, xmax, ymax)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.imgs[idx]\n",
    "        label = self.rev_synsets[name.split(\"_\")[0]]\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, label, f\"{name}.JPEG\")\n",
    "        bb_path = os.path.join(self.bb_dir, label, \"Annotation\", name.split(\"_\")[0], f\"{name}.xml\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        if bb_path in self.bb_dict.keys():\n",
    "          xmin, ymin, xmax, ymax = self.bb_dict[bb_path]\n",
    "        else:\n",
    "          # the whole image is the bounding box label, as NoneType was causing collating issue. \n",
    "          xmin = 0\n",
    "          ymin = 0\n",
    "          xmax = img.size[0]\n",
    "          ymax = img.size[1]\n",
    "        boxes = torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        # img, boxes = self.expand(img, [xmin, ymin, xmax, ymax], self.target, (0, 0, 0))\n",
    "        if self.transforms is not None:\n",
    "          img = self.transforms(img)\n",
    "\n",
    "        if self.one_hot: \n",
    "          image_id = torch.zeros(len(self.classes), dtype=torch.float32)\n",
    "          image_id [self.classes.index(label)] = 1.0\n",
    "        else:\n",
    "          image_id = torch.tensor([self.classes.index(label)])\n",
    "\n",
    "        targets = {}\n",
    "        targets[\"boxes\"] = boxes\n",
    "        targets[\"image_class\"] = image_id\n",
    "\n",
    "    \n",
    "        return img, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOicVbPgZrK1"
   },
   "outputs": [],
   "source": [
    "class GreenstandDataset(data.Dataset):\n",
    "  # We don't have labels for this yet...\n",
    "  def __init__(self, dir, device, transforms, bb_dir=None, one_hot=False):\n",
    "    self.img_dir = dir\n",
    "    self.imgs = []\n",
    "    self.bb_dir = bb_dir\n",
    "    self.transforms = transforms\n",
    "    self.classes = None # Change this when we define Greenstand class labels\n",
    "    self.one_hot = one_hot\n",
    "    self.device = device\n",
    "\n",
    "    for f, _, d in os.walk(test_path):\n",
    "      for fil in d:\n",
    "        if \"jpg\" in fil:\n",
    "          self.imgs.append(os.path.join(f, fil))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = Image.open(self.img_dir[idx])\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms (img)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAGG9gAcEgdv"
   },
   "source": [
    "## MobileNet-v2\n",
    "See [the original paper](https://arxiv.org/pdf/1704.04861.pdf) for details. This was chosen first because Torchvision has pretrained weights and the net is quite low-latency, which may be useful for user-interface image selection. First go is to simply change the output layer to predict 4 coordinates for the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2c20daa97436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mInception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m Inception_preprocessing = transforms.Compose([\n\u001b[0;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m299\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m299\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\inception.py\u001b[0m in \u001b[0;36minception_v3\u001b[1;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0moriginal_aux_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInception3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         state_dict = load_state_dict_from_url(model_urls['inception_v3_google'],\n\u001b[0;32m     55\u001b[0m                                               progress=progress)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\models\\inception.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, aux_logits, transform_input, inception_blocks)\u001b[0m\n\u001b[0;32m    109\u001b[0m                 \u001b[0mstddev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstddev\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stddev'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstddev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mrvs\u001b[1;34m(self, size, random_state)\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'random_state'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36mrvs\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;31m# by _rvs().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 980\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvals\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m_rvs\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    911\u001b[0m         \u001b[1;31m## Use basic inverse cdf algorithm for RV generation as default.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 913\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_continuous_distns.py\u001b[0m in \u001b[0;36m_ppf\u001b[1;34m(self, q, a, b)\u001b[0m\n\u001b[0;32m   7161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7162\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_ppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_truncnorm_ppf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_munp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_continuous_distns.py\u001b[0m in \u001b[0;36mvf_wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   6931\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6932\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mvf_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6933\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mvf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvf_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6935\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvectorize_decorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\greenstand\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2089\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2091\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2093\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\greenstand\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2165\u001b[0m                       for a in args]\n\u001b[0;32m   2166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2167\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_continuous_distns.py\u001b[0m in \u001b[0;36m_truncnorm_ppf\u001b[1;34m(q, a, b)\u001b[0m\n\u001b[0;32m   7079\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7080\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7081\u001b[1;33m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_truncnorm_get_delta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdelta\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_continuous_distns.py\u001b[0m in \u001b[0;36m_truncnorm_get_delta\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m   6942\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_norm_cdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0m_norm_cdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6944\u001b[1;33m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_norm_sf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0m_norm_sf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6945\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6946\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\stats\\_continuous_distns.py\u001b[0m in \u001b[0;36m_norm_sf\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_norm_sf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_norm_cdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "Inception_preprocessing = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def expand(self, pil_img, boxes, target, background_color):\n",
    "    size = {}\n",
    "    size[\"width\"], size[\"height\"] = pil_img.size\n",
    "    if max(size) == \"width\":\n",
    "        size[\"new_height\"] = int(target/size[\"width\"] * size[\"height\"])\n",
    "        size[\"new_width\"] = int(target)\n",
    "    else:\n",
    "        size[\"new_width\"] = int(target/size[\"height\"] * size[\"width\"])\n",
    "        size[\"new_height\"] = int(target)\n",
    "\n",
    "\n",
    "    x_scale = size[\"new_width\"]/size[\"width\"]\n",
    "    y_scale = size[\"new_height\"]/size[\"height\"]\n",
    "\n",
    "    pil_img = pil_img.resize((size[\"new_width\"], size[\"new_height\"]))\n",
    "\n",
    "    xmin = boxes[0]\n",
    "    ymin = boxes[1]\n",
    "    xmax = boxes[2]\n",
    "    ymax = boxes[3]\n",
    "\n",
    "    xmin = int(np.round(xmin*x_scale)) + (target - size[\"new_width\"])/2\n",
    "    ymin = int(np.round(ymin*y_scale)) + (target - size[\"new_height\"])/2\n",
    "    xmax= int(np.round(xmax*(x_scale))) + (target - size[\"new_width\"])/2\n",
    "    ymax= int(np.round(ymax*y_scale)) + (target - size[\"new_height\"])/2\n",
    "\n",
    "    result = Image.new(pil_img.mode, (target, target), background_color)\n",
    "    result.paste(pil_img, (0, (size[\"new_width\"] - size[\"new_height\"]) // 2))\n",
    "\n",
    "    return result, torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1JpU4M_iEY9"
   },
   "source": [
    "### Instantiate datasets, define loader processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Y6XzYGG3ETT"
   },
   "outputs": [],
   "source": [
    "path = \"data/imnet\"\n",
    "test_path = \"data/test_greenstand_samples\"\n",
    "model_path = \"models/ImageNet/inception/%s\"%datetime.datetime.today().date()\n",
    "if not os.path.exists(model_path):\n",
    "  os.makedirs(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqgQQtXy3b0q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating datasets in  0.7659392356872559  seconds \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_set = ImnetDataset(path, synsets, transforms=Inception_preprocessing, one_hot=False, device=_DEVICE)\n",
    "greenstand_test = GreenstandDataset(test_path, transforms=Inception_preprocessing, one_hot=False, device=_DEVICE)\n",
    "\n",
    "print (\"Finished creating datasets in \", time.time() - start, \" seconds \") # this can take ~30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk6-8FA8Ab8v"
   },
   "outputs": [],
   "source": [
    "# Helper functions \n",
    "def rmse(x, y):\n",
    "  '''\n",
    "  Root-mean squared error of two vectors of the same batch \n",
    "  '''\n",
    "  return torch.sqrt( (1/x.size()[0]) * torch.sum((x-y) **2))\n",
    "\n",
    "def iou(box_a, box_b):\n",
    "  # order is xmin, ymin, xmax, ymax \n",
    "  intersect_xmin = max(box_a[0], box_b[0])\n",
    "  intersect_ymin = max(box_a[1], box_b[1])\n",
    "  intersect_xmax = min(box_a[2], box_b[2])\n",
    "  intersect_ymax = min(box_a[3], box_b[3])\n",
    "  area_intersect = max(0, intersect_xmax - intersect_xmin) * max(0, intersect_ymax - intersect_ymin)\n",
    "\n",
    "  area_a = (box_a[3] - box_a[1]) * (box_a[2] - box_a[0])\n",
    "  area_b = (box_b[3] - box_b[1]) * (box_b[2] - box_b[0])\n",
    "  union = area_a + area_b - area_intersect\n",
    "  return area_intersect / union\n",
    "\n",
    "class Customized_Inception(nn.Module):\n",
    "  def __init__(self, pretrained_model, num_classes=10):\n",
    "    super().__init__()\n",
    "    self.pretrained = pretrained_model\n",
    "    for param in self.pretrained.parameters():\n",
    "      param.requires_grad = True\n",
    "    self.pretrained.fc = nn.Identity()\n",
    "    self._classifier_layer(num_classes)\n",
    "    self._regressor_layer()\n",
    "\n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    The model is performing a regression on bounding boxes and a classifier \n",
    "    \"\"\"\n",
    "    x = self.pretrained(x)[0]\n",
    "   \n",
    "    return self.classifier(x), self.regressor(x)\n",
    "\n",
    "  def _classifier_layer(self, num_classes):\n",
    "    \"\"\"\n",
    "    Initializes final classification layer for labeling genus, species, etc.\n",
    "    \"\"\"\n",
    "    self.classifier = nn.Sequential(\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(2048, num_classes) # 2048 is num_outputs of the last feature layer\n",
    "                      ) \n",
    "    for param in self.classifier.parameters():\n",
    "      param.requires_grad=True\n",
    "\n",
    "  def _regressor_layer(self):\n",
    "    \"\"\"\n",
    "    A bounding box output layer for predicting object location\n",
    "    This is currently designed to output exactly one bounding box\n",
    "    \"\"\"\n",
    "    self.regressor =  nn.Sequential(\n",
    "                        nn.Dropout(0.2), \n",
    "                        nn.Linear(2048, 4) # 2048 is num_outputs of the last feature layer\n",
    "                      )\n",
    "    for param in self.regressor.parameters():\n",
    "      param.requires_grad=True\n",
    "\n",
    "class ModelTrainer():\n",
    "  '''\n",
    "  An abstraction to help keep track of model parameters and run training. \n",
    "  '''\n",
    "  def __init__ (self, model, dataset, learning_rate, alpha, beta, device, batch_size, model_savepath,\n",
    "                gamma=1e-4, train_split=0.8, pin_memory=False, n_workers=0):\n",
    "    \n",
    "    self.model = model # like Customized_Inception\n",
    "    self.model_savepath = os.path.join (model_savepath, \"checkpoint.pth.tar\")\n",
    "    # Initialize device\n",
    "    self.device = device\n",
    "    if self.device == torch.device(\"cuda:0\"):\n",
    "      self.model.cuda()\n",
    "\n",
    "    # Make validation split\n",
    "    self.trainsize = int(train_split * len(dataset))\n",
    "    self.valsize = len(dataset) - self.trainsize\n",
    "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(dataset, [self.trainsize, self.valsize])\n",
    "\n",
    "    # Define data loader for training and validation\n",
    "    self.batch_size = batch_size\n",
    "    self.data_loader  = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
    "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
    "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
    "              worker_init_fn=None)\n",
    "\n",
    "    self.val_data_loader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
    "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
    "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
    "              worker_init_fn=None)\n",
    "    \n",
    "    # Loss specifications and optimizer parameter setting\n",
    "    # This is defined here so that the underlying model can be changed (i.e. hidden layers) \n",
    "    self.alpha, self.beta = alpha, beta # classification/regression loss tradeoff\n",
    "    cps = [param for param in self.model.classifier.parameters()]\n",
    "    rps = [param for param in self.model.regressor.parameters()]\n",
    "    self.optimizer = torch.optim.Adam(params=cps+rps, lr=learning_rate, weight_decay=gamma)\n",
    "    self.regression_criterion = nn.MSELoss()\n",
    "    self.classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if os.path.exists(self.model_savepath):\n",
    "      print (\"Found saved model at savepath %s\" %(self.model_savepath))\n",
    "      checkpoint = torch.load(self.model_savepath)\n",
    "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      self.start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "      self.start_epoch = 0\n",
    "\n",
    "\n",
    "  def describe_training(self):\n",
    "    print (self.trainsize, \" training examples\")\n",
    "    print (self.valsize, \" validation examples\")\n",
    "\n",
    "  def train(self, num_epochs, val_interval=1, batch_report=50, batch_lookback=10):\n",
    "    '''\n",
    "    Main function to train. \n",
    "    @param num_epochs(int): Number of epochs of training\n",
    "    @param val_interval(int): Interval epochs between validation metric\n",
    "    @param batch_report(int): Interval batches between training reports\n",
    "    @param batch_lookback(int): Number of batches to use for averaging metrics in printing\n",
    "    '''\n",
    "    num_tr_batches = np.ceil(self.trainsize/self.data_loader.batch_size)\n",
    "    num_val_batches = np.ceil(self.valsize/self.valsize)\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_rmse = []\n",
    "    epoch_iou = []\n",
    "    val_epoch_loss = []\n",
    "    val_epoch_acc = []\n",
    "    val_epoch_rmse = []\n",
    "    val_epoch_iou = []\n",
    "    print (\"Starting at epoch %d\"%self.start_epoch)\n",
    "    for epoch in range(self.start_epoch, num_epochs):\n",
    "      epoch_start = time.time()\n",
    "      print (\"=\" * 50)\n",
    "      print (\"EPOCH \", epoch)\n",
    "      batch_count = 0\n",
    "      batch_loss = []\n",
    "      batch_acc = []\n",
    "      batch_rmse = []\n",
    "      batch_iou = []\n",
    "      for batchx, batchy in self.data_loader:\n",
    "          batch_count += 1\n",
    "          # Device designation\n",
    "          if self.device == torch.device(\"cuda:0\"):\n",
    "            batchx = batchx.cuda(non_blocking=True)\n",
    "            batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
    "            batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
    "          \n",
    "          class_labels = batchy[\"image_class\"]\n",
    "          box_labels = batchy[\"boxes\"]\n",
    "\n",
    "          # Forward pass\n",
    "          \n",
    "          class_preds, box_preds = self.model.forward(batchx)\n",
    "          loss = self._loss_specification(class_preds, class_labels, box_preds, box_labels)\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "          # Metrics \n",
    "          box_rmse = rmse(box_preds, box_labels)\n",
    "          avg_box_iou = torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32))\n",
    "          class_correct = (torch.max(class_preds, 1)[1] == class_labels.squeeze()).sum()\n",
    "          acc = class_correct/float(batchx.shape[0])\n",
    "          batch_iou.append(avg_box_iou)\n",
    "          batch_rmse.append(box_rmse)\n",
    "          batch_acc.append(acc)\n",
    "          batch_loss.append(loss.data)\n",
    "\n",
    "          if batch_count % batch_report == 0 or batch_count == num_tr_batches:\n",
    "            print (\"\\nLast %d Batch Avg Metrics, Batch %d/%d\" %(batch_lookback, batch_count, num_tr_batches))\n",
    "            print (\"Total Loss: {:.3f}\".format(torch.mean(torch.as_tensor(batch_loss[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"Classification Acc: {:.3f}\".format(torch.mean(torch.as_tensor(batch_acc[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"BBox RMSE: {:.3f}\".format(torch.mean(torch.as_tensor(batch_rmse[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"Avg Bbox IoU: {:.3f} \\n\".format(torch.mean(torch.as_tensor(batch_iou[-batch_lookback:], dtype=torch.float32))))\n",
    "            torch.save({\n",
    "                          'epoch': epoch + 1,\n",
    "                          'model_state_dict': self.model.state_dict(),\n",
    "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                          }, \n",
    "                       self.model_savepath)\n",
    "            print (\"Checkpoint created\")\n",
    "      \n",
    "      #if epoch % val_interval == 0:\n",
    "      if epoch % 1 == 0:\n",
    "          print (\"VALIDATION EPOCH \", epoch)\n",
    "          batch_count = 0\n",
    "          \n",
    "          self.model.eval()\n",
    "          with torch.no_grad():\n",
    "            rmses = []\n",
    "            ious = []\n",
    "            losses = []\n",
    "            class_accs = []\n",
    "\n",
    "            for batchx, batchy in self.val_data_loader:\n",
    "                batch_count += 1\n",
    "                 # Device designation\n",
    "                if self.device == torch.device(\"cuda:0\"):\n",
    "                  batchx = batchx.cuda(non_blocking=True)\n",
    "                  batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
    "                  batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
    "                \n",
    "                class_labels = batchy[\"image_class\"]\n",
    "                box_labels = batchy[\"boxes\"]\n",
    "                class_preds, box_preds = self.model.forward(batchx)\n",
    "                losses.append(self._loss_specification(class_preds, class_labels, box_preds, box_labels))\n",
    "                class_accs.append(float((torch.max(class_preds, 1)[1] == class_labels.squeeze()).sum())/self.val_data_loader.batch_size)\n",
    "                ious.append(torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32)))\n",
    "                rmses.append(rmse(box_preds, box_labels))\n",
    "\n",
    "              \n",
    "            losses = torch.mean(torch.as_tensor(losses, dtype=torch.float32))\n",
    "            class_accs = torch.mean(torch.as_tensor(class_accs, dtype=torch.float32))\n",
    "            box_rmse = torch.mean(torch.as_tensor(rmses, dtype=torch.float32))\n",
    "            avg_box_iou = torch.mean(torch.as_tensor(ious, dtype=torch.float32))\n",
    "            val_epoch_loss.append(losses)\n",
    "            val_epoch_acc.append(class_accs)\n",
    "            val_epoch_rmse.append(box_rmse)\n",
    "            val_epoch_iou.append(avg_box_iou)\n",
    "\n",
    "            # We can change this to be epoch wise or not averaged over all batches\n",
    "            print (\"Batch Average Val Loss: {:.3f}\".format(losses))\n",
    "            print (\"Batch Avg Val Classification Acc: {:.3f}\".format(class_accs))\n",
    "            print (\"Batch Avg Val BBox RMSE: {:.3f}\".format(box_rmse))\n",
    "            print (\"Batch Avg Avg Bbox IoU: {:.3f} \\n\".format(avg_box_iou))\n",
    "          self.model.train()\n",
    "      epoch_loss.append(torch.mean(torch.as_tensor(batch_loss, dtype=torch.float32)))\n",
    "      epoch_acc.append(torch.mean(torch.as_tensor(batch_acc, dtype=torch.float32)))\n",
    "      epoch_iou.append(torch.mean(torch.as_tensor(batch_iou, dtype=torch.float32)))\n",
    "      epoch_rmse.append(torch.mean(torch.as_tensor(batch_rmse, dtype=torch.float32)))\n",
    "  \n",
    "      print (\"Epoch \", epoch + 1, \" finished in \", time.time() - epoch_start)\n",
    "    tr_metric_dict = {\"Loss\": epoch_loss, \"Acc\": epoch_acc, \"IoU\": epoch_iou, \"RMSE\": epoch_rmse}\n",
    "    val_metric_dict = {\"Loss\": val_epoch_loss, \"Acc\": val_epoch_acc, \"IoU\": val_epoch_iou, \"RMSE\": val_epoch_rmse}\n",
    "    #torch.save({\n",
    "    #          'epoch': epoch + 1,\n",
    "    #          'model_state_dict': self.model.state_dict(),\n",
    "    #          'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "    #          'tr_metric_dict': tr_metric_dict, \n",
    "    #          'val_metric_dict': val_metric_dict\n",
    "    #          }, \n",
    "    #        self.model_savepath)\n",
    "    print (\"Final checkpoint created. Model dict and metrics saved. \")\n",
    "    return self.model, tr_metric_dict, val_metric_dict\n",
    "\n",
    "\n",
    "  def _loss_specification(self, class_preds, class_labels, box_preds, box_labels):\n",
    "    bounding_box_error = self.regression_criterion(box_preds, box_labels) # bounding box regression error\n",
    "    # preds_flat = torch.max(class_preds, dim=1)[1] # BSIZE x 1 (index)\n",
    "\n",
    "    classification_loss = self.classification_criterion(class_preds, class_labels.view(-1)) # classification error\n",
    "    return self.alpha * classification_loss + self.beta * bounding_box_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "U6TrQ9DDL6HD",
    "outputId": "8d2feec1-7491-4a71-b29d-e0e1d4a6194c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found saved model at savepath models/ImageNet/inception/2020-11-22\\checkpoint.pth.tar\n",
      "23083  training examples\n",
      "5771  validation examples\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "_N_WORKERS = 0\n",
    "_PIN_MEM = True\n",
    "\n",
    "mob_model = Customized_Inception(pretrained_model=Inception, num_classes=28)\n",
    "trainer = ModelTrainer(mob_model, \n",
    "                       data_set, \n",
    "                       train_split=0.8,\n",
    "                       learning_rate=0.0005,\n",
    "                       batch_size = 8,\n",
    "                       alpha=0.9, # classification loss weight \n",
    "                       beta=0.1, # regression loss weight\n",
    "                       device=_DEVICE, \n",
    "                       pin_memory=_PIN_MEM,\n",
    "                       n_workers=_N_WORKERS,\n",
    "                       model_savepath=model_path)\n",
    "print (trainer.describe_training())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl5ULX4vu4YB"
   },
   "source": [
    "### Check model structure and parameters\n",
    "Regressor and classifier (final layer) should require grad. Others should not. Optimizer should be set only to those regressor and classifier variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "72xHKgN4RLah",
    "outputId": "b7bb0805-e3a9-4626-859a-75ca803ecdd7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at epoch 2\n",
      "==================================================\n",
      "EPOCH  2\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 200/2886\n",
      "Total Loss: 2344.719\n",
      "Classification Acc: 0.225\n",
      "BBox RMSE: 276.072\n",
      "Avg Bbox IoU: 0.530 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 400/2886\n",
      "Total Loss: 1598.697\n",
      "Classification Acc: 0.112\n",
      "BBox RMSE: 234.816\n",
      "Avg Bbox IoU: 0.556 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 600/2886\n",
      "Total Loss: 2254.305\n",
      "Classification Acc: 0.138\n",
      "BBox RMSE: 267.983\n",
      "Avg Bbox IoU: 0.476 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 800/2886\n",
      "Total Loss: 3148.052\n",
      "Classification Acc: 0.225\n",
      "BBox RMSE: 293.190\n",
      "Avg Bbox IoU: 0.561 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 1000/2886\n",
      "Total Loss: 3328.482\n",
      "Classification Acc: 0.125\n",
      "BBox RMSE: 304.808\n",
      "Avg Bbox IoU: 0.500 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 1200/2886\n",
      "Total Loss: 3163.103\n",
      "Classification Acc: 0.225\n",
      "BBox RMSE: 301.011\n",
      "Avg Bbox IoU: 0.478 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 1400/2886\n",
      "Total Loss: 2828.597\n",
      "Classification Acc: 0.125\n",
      "BBox RMSE: 291.578\n",
      "Avg Bbox IoU: 0.480 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 1600/2886\n",
      "Total Loss: 2812.556\n",
      "Classification Acc: 0.200\n",
      "BBox RMSE: 282.798\n",
      "Avg Bbox IoU: 0.522 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 1800/2886\n",
      "Total Loss: 3054.126\n",
      "Classification Acc: 0.200\n",
      "BBox RMSE: 306.524\n",
      "Avg Bbox IoU: 0.528 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 2000/2886\n",
      "Total Loss: 2455.704\n",
      "Classification Acc: 0.162\n",
      "BBox RMSE: 278.508\n",
      "Avg Bbox IoU: 0.466 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 2200/2886\n",
      "Total Loss: 2864.487\n",
      "Classification Acc: 0.150\n",
      "BBox RMSE: 315.315\n",
      "Avg Bbox IoU: 0.444 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 2400/2886\n",
      "Total Loss: 1751.141\n",
      "Classification Acc: 0.175\n",
      "BBox RMSE: 236.754\n",
      "Avg Bbox IoU: 0.502 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 2600/2886\n",
      "Total Loss: 1692.303\n",
      "Classification Acc: 0.125\n",
      "BBox RMSE: 232.631\n",
      "Avg Bbox IoU: 0.502 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 2800/2886\n",
      "Total Loss: 3900.694\n",
      "Classification Acc: 0.262\n",
      "BBox RMSE: 356.609\n",
      "Avg Bbox IoU: 0.470 \n",
      "\n",
      "Checkpoint created\n",
      "\n",
      "Last 10 Batch Avg Metrics, Batch 2886/2886\n",
      "Total Loss: 1952.878\n",
      "Classification Acc: 0.221\n",
      "BBox RMSE: 251.624\n",
      "Avg Bbox IoU: 0.445 \n",
      "\n",
      "Checkpoint created\n",
      "VALIDATION EPOCH  2\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keesb\\anaconda3\\envs\\greenstand\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([8, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e499685b048f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_report\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-e4541d1106a3>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, num_epochs, val_interval, batch_report, batch_lookback)\u001b[0m\n\u001b[0;32m    210\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss_specification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m                 \u001b[0mclass_accs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_data_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                 \u001b[0mious\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox_preds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-e4541d1106a3>\u001b[0m in \u001b[0;36m_loss_specification\u001b[1;34m(self, class_preds, class_labels, box_preds, box_labels)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;31m# preds_flat = torch.max(class_preds, dim=1)[1] # BSIZE x 1 (index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[0mclassification_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# classification error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mclassification_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbounding_box_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\greenstand\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\greenstand\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m    916\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\greenstand\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\greenstand\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=100,batch_report=200)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bounding_box.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c202d3e736f460a98043fcae9a23cff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36796867a0b748ef92c00db0ab37e7d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46718ab62dd64eccb199385178852625": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c202d3e736f460a98043fcae9a23cff",
      "max": 14212972,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9aeb30ec645f433a8cae01641ed4daba",
      "value": 14212972
     }
    },
    "5565e2ecf6f14b29a7f4564a010395b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93efecf468194e66a4dd8209bdb3ced0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36796867a0b748ef92c00db0ab37e7d3",
      "placeholder": "​",
      "style": "IPY_MODEL_5565e2ecf6f14b29a7f4564a010395b6",
      "value": " 13.6M/13.6M [13:35&lt;00:00, 17.4kB/s]"
     }
    },
    "9aeb30ec645f433a8cae01641ed4daba": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b830eca0137c4435ac4986bbaa3baf3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_46718ab62dd64eccb199385178852625",
       "IPY_MODEL_93efecf468194e66a4dd8209bdb3ced0"
      ],
      "layout": "IPY_MODEL_bf34dd476f1b441c9ecf0ef16647f9d6"
     }
    },
    "bf34dd476f1b441c9ecf0ef16647f9d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
