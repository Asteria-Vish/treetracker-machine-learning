{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bounding_box.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b830eca0137c4435ac4986bbaa3baf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf34dd476f1b441c9ecf0ef16647f9d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46718ab62dd64eccb199385178852625",
              "IPY_MODEL_93efecf468194e66a4dd8209bdb3ced0"
            ]
          }
        },
        "bf34dd476f1b441c9ecf0ef16647f9d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46718ab62dd64eccb199385178852625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9aeb30ec645f433a8cae01641ed4daba",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c202d3e736f460a98043fcae9a23cff"
          }
        },
        "93efecf468194e66a4dd8209bdb3ced0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5565e2ecf6f14b29a7f4564a010395b6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [13:35&lt;00:00, 17.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36796867a0b748ef92c00db0ab37e7d3"
          }
        },
        "9aeb30ec645f433a8cae01641ed4daba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c202d3e736f460a98043fcae9a23cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5565e2ecf6f14b29a7f4564a010395b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36796867a0b748ef92c00db0ab37e7d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhomb/greenstand_data_analysis/blob/master/imnet/bounding_box.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV4G09d8nrxd",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwiQQCjdvnqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "561e24eb-ebad-4238-94cd-52f70c5ebe7b"
      },
      "source": [
        "\n",
        "# how to access GDrive https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\n",
        "from google.colab import files, drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "gdir = os.path.join(os.getcwd(), \"drive\", \"My Drive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/4AFEkI9BC8sfUcEpbVJJOtOrZXP0rO_fl3LYAMbs45z0Ya5cT77ljnQ\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoKegeKl28VU",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MojfFLIl2-En",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f484d10-bdfc-4b31-cb70-c23bd3a60eff"
      },
      "source": [
        "# Torch Dataset and IMNet Loading\n",
        "import torch\n",
        "from xml.etree import ElementTree\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageDraw\n",
        "from  collections import OrderedDict\n",
        "import numpy as np\n",
        "\n",
        "# Model development and training\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Filesystem and parallelization\n",
        "import os\n",
        "import multiprocessing\n",
        "\n",
        "# Utility \n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Constants for parallelization\n",
        "_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# from ImageNet\n",
        "synsets = {\n",
        "    \"judas\": \"n12513613\",\n",
        "    \"palm\": \"n12582231\",\n",
        "    \"pine\": \"n11608250\",\n",
        "    \"china tree\": \"n12741792\",\n",
        "    \"fig\": \"n12401684\",\n",
        "    \"cabbage\": \"n12478768\",\n",
        "    \"cacao\": \"n12201580\",\n",
        "    \"kapok\": \"n12190410\",\n",
        "    \"iron\": \"n12317296\",\n",
        "    \"linden\": \"n12202936\",\n",
        "    \"pepper\": \"n12765115\",\n",
        "    \"rain\": \"n11759853\",\n",
        "    \"dita\": \"n11770256\",\n",
        "    \"alder\": \"n12284262\",\n",
        "    \"silk\": \"n11759404\",\n",
        "    \"coral\": \"n12527738\",\n",
        "    \"huisache\": \"n11757851\",\n",
        "    \"fringe\": \"n12302071\",\n",
        "    \"dogwood\": \"n12946849\",\n",
        "    \"cork\": \"n12713866\",\n",
        "    \"ginkgo\": \"n11664418\",\n",
        "    \"golden shower\": \"n12492106\",\n",
        "    \"balata\": \"n12774299\",\n",
        "    \"baobab\": \"n12189987\",\n",
        "    \"sorrel\": \"n12242409\",\n",
        "    \"Japanese pagoda\": \"n12570394\",\n",
        "    \"Kentucky coffee\": \"n12496427\",\n",
        "    \"Logwood\": \"n12496949\"\n",
        "}\n",
        "\n",
        "\n",
        "print (\"Device:\" , _DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wshbg0pg3IHX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Dataset Creation\n",
        "Define datasets for ImageNet and Greenstand sources. Greenstand species classes are yet unlabeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ2MNziQ3B-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImnetDataset(data.Dataset):\n",
        "    \n",
        "    # initialise function of class\n",
        "    def __init__(self, dir, synsets, transforms=None, device=None, one_hot=False):\n",
        "        # the data directories\n",
        "        self.img_dir = os.path.join(dir, \"original_images\")\n",
        "        self.bb_dir = os.path.join(dir, \"bounding_boxes\")\n",
        "\n",
        "        #synsets library to get the associated class\n",
        "        self.synsets = synsets\n",
        "        self.rev_synsets = {y:x for x,y in zip(synsets.keys(), synsets.values())}\n",
        "        self.classes = list(self.synsets.keys())\n",
        "        # self.target = target\n",
        "\n",
        "        self.one_hot = one_hot\n",
        "        self.imgs = []\n",
        "\n",
        "        for i in self.classes:\n",
        "          temp_imgs = list(sorted(os.listdir(os.path.join(self.img_dir, i))))\n",
        "          for img_path in temp_imgs:\n",
        "            #in every directory the \"tar\" file is still present\n",
        "            if not \"tar\" in img_path:\n",
        "              name = img_path.split('.')[0]\n",
        "              self.imgs.append(name)\n",
        "\n",
        "        self.bb_dict = {}\n",
        "        for f, _, d in os.walk(self.bb_dir):\n",
        "          for file in d:\n",
        "            if os.path.splitext(file)[1] == \".xml\":\n",
        "              tree = ElementTree.parse(os.path.join(f, file))\n",
        "              root = tree.getroot()\n",
        "              obj = root.find(\"object\")\n",
        "              b = obj.find(\"bndbox\")\n",
        "              xmin = int(b.find(\"xmin\").text)\n",
        "              ymin = int(b.find(\"ymin\").text)\n",
        "              xmax = int(b.find(\"xmax\").text)\n",
        "              ymax = int(b.find(\"ymax\").text)\n",
        "              self.bb_dict[os.path.join(f, file)] =  (xmin, ymin, xmax, ymax)\n",
        "\n",
        "        self.transforms = transforms\n",
        "        self.device = device\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.imgs[idx]\n",
        "        label = self.rev_synsets[name.split(\"_\")[0]]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, label, f\"{name}.JPEG\")\n",
        "        bb_path = os.path.join(self.bb_dir, label, \"Annotation\", name.split(\"_\")[0], f\"{name}.xml\")\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "\n",
        "        if bb_path in self.bb_dict.keys():\n",
        "          xmin, ymin, xmax, ymax = self.bb_dict[bb_path]\n",
        "        else:\n",
        "          # the whole image is the bounding box label, as NoneType was causing collating issue. \n",
        "          xmin = 0\n",
        "          ymin = 0\n",
        "          xmax = img.size[0]\n",
        "          ymax = img.size[1]\n",
        "        boxes = torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
        "\n",
        "        \n",
        "        # img, boxes = self.expand(img, [xmin, ymin, xmax, ymax], self.target, (0, 0, 0))\n",
        "        if self.transforms is not None:\n",
        "          img = self.transforms(img)\n",
        "\n",
        "        if self.one_hot: \n",
        "          image_id = torch.zeros(len(self.classes), dtype=torch.float32)\n",
        "          image_id [self.classes.index(label)] = 1.0\n",
        "        else:\n",
        "          image_id = torch.tensor([self.classes.index(label)])\n",
        "\n",
        "        targets = {}\n",
        "        targets[\"boxes\"] = boxes\n",
        "        targets[\"image_class\"] = image_id\n",
        "\n",
        "    \n",
        "        return img, targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOicVbPgZrK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreenstandDataset(data.Dataset):\n",
        "  # We don't have labels for this yet...\n",
        "  def __init__(self, dir, device, transforms, bb_dir=None, one_hot=False):\n",
        "    self.img_dir = dir\n",
        "    self.imgs = []\n",
        "    self.bb_dir = bb_dir\n",
        "    self.transforms = transforms\n",
        "    self.classes = None # Change this when we define Greenstand class labels\n",
        "    self.one_hot = one_hot\n",
        "    self.device = device\n",
        "\n",
        "    for f, _, d in os.walk(test_path):\n",
        "      for fil in d:\n",
        "        if \"jpg\" in fil:\n",
        "          self.imgs.append(os.path.join(f, fil))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = Image.open(self.img_dir[idx])\n",
        "    if self.transforms is not None:\n",
        "      img = self.transforms (img)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAGG9gAcEgdv",
        "colab_type": "text"
      },
      "source": [
        "## MobileNet-v2\n",
        "See [the original paper](https://arxiv.org/pdf/1704.04861.pdf) for details. This was chosen first because Torchvision has pretrained weights and the net is quite low-latency, which may be useful for user-interface image selection. First go is to simply change the output layer to predict 4 coordinates for the bounding box."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g48vTx64Wdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "b830eca0137c4435ac4986bbaa3baf3a",
            "bf34dd476f1b441c9ecf0ef16647f9d6",
            "46718ab62dd64eccb199385178852625",
            "93efecf468194e66a4dd8209bdb3ced0",
            "9aeb30ec645f433a8cae01641ed4daba",
            "0c202d3e736f460a98043fcae9a23cff",
            "5565e2ecf6f14b29a7f4564a010395b6",
            "36796867a0b748ef92c00db0ab37e7d3"
          ]
        },
        "outputId": "d9947a61-7e71-4073-e0be-c285b682114c"
      },
      "source": [
        "mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "# Preprocessing required for MobileNet v2\n",
        "\n",
        "    \n",
        "def expand(self, pil_img, boxes, target, background_color):\n",
        "    size = {}\n",
        "    size[\"width\"], size[\"height\"] = pil_img.size\n",
        "    if max(size) == \"width\":\n",
        "        size[\"new_height\"] = int(target/size[\"width\"] * size[\"height\"])\n",
        "        size[\"new_width\"] = int(target)\n",
        "    else:\n",
        "        size[\"new_width\"] = int(target/size[\"height\"] * size[\"width\"])\n",
        "        size[\"new_height\"] = int(target)\n",
        "\n",
        "\n",
        "    x_scale = size[\"new_width\"]/size[\"width\"]\n",
        "    y_scale = size[\"new_height\"]/size[\"height\"]\n",
        "\n",
        "    pil_img = pil_img.resize((size[\"new_width\"], size[\"new_height\"]))\n",
        "\n",
        "    xmin = boxes[0]\n",
        "    ymin = boxes[1]\n",
        "    xmax = boxes[2]\n",
        "    ymax = boxes[3]\n",
        "\n",
        "    xmin = int(np.round(xmin*x_scale)) + (target - size[\"new_width\"])/2\n",
        "    ymin = int(np.round(ymin*y_scale)) + (target - size[\"new_height\"])/2\n",
        "    xmax= int(np.round(xmax*(x_scale))) + (target - size[\"new_width\"])/2\n",
        "    ymax= int(np.round(ymax*y_scale)) + (target - size[\"new_height\"])/2\n",
        "\n",
        "    result = Image.new(pil_img.mode, (target, target), background_color)\n",
        "    result.paste(pil_img, (0, (size[\"new_width\"] - size[\"new_height\"]) // 2))\n",
        "\n",
        "    return result, torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
        "\n",
        "mobilenet_preprocessing = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b830eca0137c4435ac4986bbaa3baf3a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=14212972.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1JpU4M_iEY9",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate datasets, define loader processes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y6XzYGG3ETT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/data/imnet\"\n",
        "test_path = \"/content/drive/My Drive/data/test_greenstand_samples\"\n",
        "model_path = \"/content/drive/My Drive/models/ImageNet/mobilenet/%s\"%datetime.datetime.today().date()\n",
        "if not os.path.exists(model_path):\n",
        "  os.makedirs(model_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqgQQtXy3b0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = time.time()\n",
        "data_set = ImnetDataset(path, synsets, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE)\n",
        "greenstand_test = GreenstandDataset(test_path, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE)\n",
        "\n",
        "print (\"Finished creating datasets in \", time.time() - start, \" seconds \") # this can take ~30 minutes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk6-8FA8Ab8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions \n",
        "def rmse(x, y):\n",
        "  '''\n",
        "  Root-mean squared error of two vectors of the same batch \n",
        "  '''\n",
        "  return torch.sqrt( (1/x.size()[0]) * torch.sum((x-y) **2))\n",
        "\n",
        "def iou(box_a, box_b):\n",
        "  # order is xmin, ymin, xmax, ymax \n",
        "  intersect_xmin = max(box_a[0], box_b[0])\n",
        "  intersect_ymin = max(box_a[1], box_b[1])\n",
        "  intersect_xmax = min(box_a[2], box_b[2])\n",
        "  intersect_ymax = min(box_a[3], box_b[3])\n",
        "  area_intersect = max(0, intersect_xmax - intersect_xmin) * max(0, intersect_ymax - intersect_ymin)\n",
        "\n",
        "  area_a = (box_a[3] - box_a[1]) * (box_a[2] - box_a[0])\n",
        "  area_b = (box_b[3] - box_b[1]) * (box_b[2] - box_b[0])\n",
        "  union = area_a + area_b - area_intersect\n",
        "  return area_intersect / union\n",
        "\n",
        "class Customized_MobileNet(nn.Module):\n",
        "  def __init__(self, pretrained_model, num_classes=10):\n",
        "    super().__init__()\n",
        "    self.pretrained = pretrained_model\n",
        "    for param in self.pretrained.parameters():\n",
        "      param.requires_grad = False\n",
        "    self.pretrained.classifier = nn.Identity()\n",
        "    self._classifier_layer(num_classes)\n",
        "    self._regressor_layer()\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    The model is performing a regression on bounding boxes and a classifier \n",
        "    \"\"\"\n",
        "    return (self.classifier(self.pretrained(x)), self.regressor(self.pretrained(x)))\n",
        "\n",
        "  def _classifier_layer(self, num_classes):\n",
        "    \"\"\"\n",
        "    Initializes final classification layer for labeling genus, species, etc.\n",
        "    \"\"\"\n",
        "    self.classifier = nn.Sequential(\n",
        "                        nn.Dropout(0.2),\n",
        "                        nn.Linear(1280, num_classes) # 1280 is num_outputs of the last feature layer\n",
        "                      ) \n",
        "    for param in self.classifier.parameters():\n",
        "      param.requires_grad=True\n",
        "\n",
        "  def _regressor_layer(self):\n",
        "    \"\"\"\n",
        "    A bounding box output layer for predicting object location\n",
        "    This is currently designed to output exactly one bounding box\n",
        "    \"\"\"\n",
        "    self.regressor =  nn.Sequential(\n",
        "                        nn.Dropout(0.2), \n",
        "                        nn.Linear(1280, 4) # 1280 is num_outputs of the last feature layer\n",
        "                      )\n",
        "    for param in self.regressor.parameters():\n",
        "      param.requires_grad=True\n",
        "\n",
        "class ModelTrainer():\n",
        "  '''\n",
        "  An abstraction to help keep track of model parameters and run training. \n",
        "  '''\n",
        "  def __init__ (self, model, dataset, learning_rate, alpha, beta, device, batch_size, model_savepath,\n",
        "                gamma=1e-4, train_split=0.8, pin_memory=False, n_workers=0):\n",
        "    \n",
        "    self.model = model # like Customized_MobileNet\n",
        "    self.model_savepath = os.path.join (model_savepath, \"checkpoint.pth.tar\")\n",
        "    # Initialize device\n",
        "    self.device = device\n",
        "    if self.device == torch.device(\"cuda:0\"):\n",
        "      self.model.cuda()\n",
        "\n",
        "    # Make validation split\n",
        "    self.trainsize = int(train_split * len(dataset))\n",
        "    self.valsize = len(dataset) - self.trainsize\n",
        "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(dataset, [self.trainsize, self.valsize])\n",
        "\n",
        "    # Define data loader for training and validation\n",
        "    self.batch_size = batch_size\n",
        "    self.data_loader  = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
        "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
        "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "\n",
        "    self.val_data_loader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
        "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
        "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
        "              worker_init_fn=None)\n",
        "    \n",
        "    # Loss specifications and optimizer parameter setting\n",
        "    # This is defined here so that the underlying model can be changed (i.e. hidden layers) \n",
        "    self.alpha, self.beta = alpha, beta # classification/regression loss tradeoff\n",
        "    cps = [param for param in self.model.classifier.parameters()]\n",
        "    rps = [param for param in self.model.regressor.parameters()]\n",
        "    self.optimizer = torch.optim.Adam(params=cps+rps, lr=learning_rate, weight_decay=gamma)\n",
        "    self.regression_criterion = nn.MSELoss()\n",
        "    self.classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if os.path.exists(self.model_savepath):\n",
        "      print (\"Found saved model at savepath %s\" %(self.model_savepath))\n",
        "      checkpoint = torch.load(self.model_savepath)\n",
        "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      self.start_epoch = checkpoint['epoch']\n",
        "    else:\n",
        "      self.start_epoch = 0\n",
        "\n",
        "\n",
        "  def describe_training(self):\n",
        "    print (self.trainsize, \" training examples\")\n",
        "    print (self.valsize, \" validation examples\")\n",
        "\n",
        "  def train(self, num_epochs, val_interval=1, batch_report=50, batch_lookback=10):\n",
        "    '''\n",
        "    Main function to train. \n",
        "    @param num_epochs(int): Number of epochs of training\n",
        "    @param val_interval(int): Interval epochs between validation metric\n",
        "    @param batch_report(int): Interval batches between training reports\n",
        "    @param batch_lookback(int): Number of batches to use for averaging metrics in printing\n",
        "    '''\n",
        "    num_tr_batches = np.ceil(self.trainsize/self.data_loader.batch_size)\n",
        "    num_val_batches = np.ceil(self.valsize/self.valsize)\n",
        "    epoch_loss = []\n",
        "    epoch_acc = []\n",
        "    epoch_rmse = []\n",
        "    epoch_iou = []\n",
        "    val_epoch_loss = []\n",
        "    val_epoch_acc = []\n",
        "    val_epoch_rmse = []\n",
        "    val_epoch_iou = []\n",
        "    print (\"Starting at epoch %d\"%self.start_epoch)\n",
        "    for epoch in range(self.start_epoch, num_epochs):\n",
        "      epoch_start = time.time()\n",
        "      print (\"=\" * 50)\n",
        "      print (\"EPOCH \", epoch)\n",
        "      batch_count = 0\n",
        "      batch_loss = []\n",
        "      batch_acc = []\n",
        "      batch_rmse = []\n",
        "      batch_iou = []\n",
        "      for batchx, batchy in self.data_loader:\n",
        "          batch_count += 1\n",
        "          # Device designation\n",
        "          if self.device == torch.device(\"cuda:0\"):\n",
        "            batchx = batchx.cuda(non_blocking=True)\n",
        "            batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
        "            batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
        "          \n",
        "          class_labels = batchy[\"image_class\"]\n",
        "          box_labels = batchy[\"boxes\"]\n",
        "\n",
        "          # Forward pass\n",
        "          class_preds, box_preds = self.model.forward(batchx)\n",
        "          loss = self._loss_specification(class_preds, class_labels, box_preds, box_labels)\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "\n",
        "          # Metrics \n",
        "          box_rmse = rmse(box_preds, box_labels)\n",
        "          avg_box_iou = torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32))\n",
        "          class_correct = (torch.max(class_preds, 1)[1] == class_labels.squeeze()).sum()\n",
        "          acc = class_correct/float(batchx.shape[0])\n",
        "          batch_iou.append(avg_box_iou)\n",
        "          batch_rmse.append(box_rmse)\n",
        "          batch_acc.append(acc)\n",
        "          batch_loss.append(loss.data)\n",
        "\n",
        "          if batch_count % batch_report == 0 or batch_count == num_tr_batches:\n",
        "            print (\"\\nLast %d Batch Avg Metrics, Batch %d/%d\" %(batch_lookback, batch_count, num_tr_batches))\n",
        "            print (\"Total Loss: {:.3f}\".format(torch.mean(torch.as_tensor(batch_loss[-batch_lookback:], dtype=torch.float32))))\n",
        "            print (\"Classification Acc: {:.3f}\".format(torch.mean(torch.as_tensor(batch_acc[-batch_lookback:], dtype=torch.float32))))\n",
        "            print (\"BBox RMSE: {:.3f}\".format(torch.mean(torch.as_tensor(batch_rmse[-batch_lookback:], dtype=torch.float32))))\n",
        "            print (\"Avg Bbox IoU: {:.3f} \\n\".format(torch.mean(torch.as_tensor(batch_iou[-batch_lookback:], dtype=torch.float32))))\n",
        "            torch.save({\n",
        "                          'epoch': epoch + 1,\n",
        "                          'model_state_dict': self.model.state_dict(),\n",
        "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                          }, \n",
        "                       self.model_savepath)\n",
        "            print (\"Checkpoint created\")\n",
        "      \n",
        "      if epoch % val_interval == 0:\n",
        "          print (\"VALIDATION EPOCH \", epoch)\n",
        "          batch_count = 0\n",
        "          \n",
        "          self.model.eval()\n",
        "          with torch.no_grad():\n",
        "            rmses = []\n",
        "            ious = []\n",
        "            losses = []\n",
        "            class_accs = []\n",
        "\n",
        "            for batchx, batchy in self.val_data_loader:\n",
        "                batch_count += 1\n",
        "                 # Device designation\n",
        "                if self.device == torch.device(\"cuda:0\"):\n",
        "                  batchx = batchx.cuda(non_blocking=True)\n",
        "                  batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
        "                  batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
        "                \n",
        "                class_labels = batchy[\"image_class\"]\n",
        "                box_labels = batchy[\"boxes\"]\n",
        "                class_preds, box_preds = self.model.forward(batchx)\n",
        "                losses.append(self._loss_specification(class_preds, class_labels, box_preds, box_labels).data)\n",
        "                class_accs.append(float((torch.max(class_preds, 1)[1] == class_labels.squeeze()).sum())/self.val_data_loader.batch_size)\n",
        "                ious.append(torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32)))\n",
        "                rmses.append(rmse(box_preds, box_labels))\n",
        "\n",
        "              \n",
        "            losses = torch.mean(torch.as_tensor(losses, dtype=torch.float32))\n",
        "            class_accs = torch.mean(torch.as_tensor(class_accs, dtype=torch.float32))\n",
        "            box_rmse = torch.mean(torch.as_tensor(rmses, dtype=torch.float32))\n",
        "            avg_box_iou = torch.mean(torch.as_tensor(ious, dtype=torch.float32))\n",
        "            val_epoch_loss.append(losses)\n",
        "            val_epoch_acc.append(class_accs)\n",
        "            val_epoch_rmse.append(box_rmse)\n",
        "            val_epoch_iou.append(avg_box_iou)\n",
        "\n",
        "            # We can change this to be epoch wise or not averaged over all batches\n",
        "            print (\"Batch Average Val Loss: {:.3f}\".format(losses))\n",
        "            print (\"Batch Avg Val Classification Acc: {:.3f}\".format(class_accs))\n",
        "            print (\"Batch Avg Val BBox RMSE: {:.3f}\".format(box_rmse))\n",
        "            print (\"Batch Avg Avg Bbox IoU: {:.3f} \\n\".format(avg_box_iou))\n",
        "          self.model.train()\n",
        "      epoch_loss.append(torch.mean(torch.as_tensor(batch_loss, dtype=torch.float32)))\n",
        "      epoch_acc.append(torch.mean(torch.as_tensor(batch_acc, dtype=torch.float32)))\n",
        "      epoch_iou.append(torch.mean(torch.as_tensor(batch_iou, dtype=torch.float32)))\n",
        "      epoch_rmse.append(torch.mean(torch.as_tensor(batch_rmse, dtype=torch.float32)))\n",
        "  \n",
        "      print (\"Epoch \", epoch + 1, \" finished in \", time.time() - epoch_start)\n",
        "    tr_metric_dict = {\"Loss\": epoch_loss, \"Acc\": epoch_acc, \"IoU\": epoch_iou, \"RMSE\": epoch_rmse}\n",
        "    val_metric_dict = {\"Loss\": val_epoch_loss, \"Acc\": val_epoch_acc, \"IoU\": val_epoch_iou, \"RMSE\": val_epoch_rmse}\n",
        "    torch.save({\n",
        "              'epoch': epoch + 1,\n",
        "              'model_state_dict': self.model.state_dict(),\n",
        "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "              'tr_metric_dict': tr_metric_dict, \n",
        "              'val_metric_dict': val_metric_dict\n",
        "              }, \n",
        "            self.model_savepath)\n",
        "    print (\"Final checkpoint created. Model dict and metrics saved. \")\n",
        "    return self.model, tr_metric_dict, val_metric_dict\n",
        "\n",
        "\n",
        "  def _loss_specification(self, class_preds, class_labels, box_preds, box_labels):\n",
        "    bounding_box_error = self.regression_criterion(box_preds, box_labels) # bounding box regression error\n",
        "    # preds_flat = torch.max(class_preds, dim=1)[1] # BSIZE x 1 (index)\n",
        "    classification_loss = self.classification_criterion(class_preds, class_labels.view(-1)) # classification error\n",
        "    return self.alpha * classification_loss + self.beta * bounding_box_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6TrQ9DDL6HD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8d2feec1-7491-4a71-b29d-e0e1d4a6194c"
      },
      "source": [
        "torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "_N_WORKERS = 0\n",
        "_PIN_MEM = True\n",
        "\n",
        "mob_model = Customized_MobileNet(pretrained_model=mobilenet, num_classes=28)\n",
        "trainer = ModelTrainer(mob_model, \n",
        "                       data_set, \n",
        "                       train_split=0.8,\n",
        "                       learning_rate=0.002,\n",
        "                       batch_size = 32,\n",
        "                       alpha=0.2, # classification loss weight \n",
        "                       beta=0.8, # regression loss weight\n",
        "                       device=_DEVICE, \n",
        "                       pin_memory=_PIN_MEM,\n",
        "                       n_workers=_N_WORKERS,\n",
        "                       model_savepath=model_path)\n",
        "print (trainer.describe_training())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23085  training examples\n",
            "5772  validation examples\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xl5ULX4vu4YB",
        "colab_type": "text"
      },
      "source": [
        "### Check model structure and parameters\n",
        "Regressor and classifier (final layer) should require grad. Others should not. Optimizer should be set only to those regressor and classifier variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdO1y3kTtfpM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "0a146472-4a93-4774-da11-7e6d14eae4a4"
      },
      "source": [
        "print (\"Regressor params\")\n",
        "print ([p for p in mob_model.regressor.parameters()])\n",
        "\n",
        "print (\"Classifier params\")\n",
        "print ([p for p in mob_model.classifier.parameters()])\n",
        "\n",
        "# Should output just two layers (4 variables total)\n",
        "for param in mob_model.parameters():\n",
        "  if param.requires_grad:\n",
        "    print (param.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Regressor params\n",
            "[Parameter containing:\n",
            "tensor([[ 1.6537e-02, -1.0734e-02, -1.0267e-02,  ...,  2.4273e-02,\n",
            "         -1.0482e-02, -2.4949e-02],\n",
            "        [-1.8666e-02,  4.3694e-03,  1.4824e-02,  ...,  4.8098e-03,\n",
            "          8.4475e-03, -6.0068e-04],\n",
            "        [ 2.6587e-03, -2.8985e-03,  7.2081e-03,  ...,  1.3932e-02,\n",
            "          4.9859e-03, -1.8791e-02],\n",
            "        [-2.5874e-02,  8.8736e-03,  1.4604e-02,  ...,  3.8725e-05,\n",
            "         -5.7580e-03, -1.4739e-02]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.0137, 0.0058, 0.0021, 0.0037], device='cuda:0', requires_grad=True)]\n",
            "Classifier params\n",
            "[Parameter containing:\n",
            "tensor([[-0.0219,  0.0032, -0.0052,  ..., -0.0260, -0.0169, -0.0185],\n",
            "        [-0.0229,  0.0061, -0.0253,  ..., -0.0146,  0.0052,  0.0035],\n",
            "        [ 0.0228, -0.0158, -0.0019,  ..., -0.0021,  0.0033,  0.0258],\n",
            "        ...,\n",
            "        [-0.0093,  0.0067,  0.0028,  ..., -0.0248, -0.0062,  0.0035],\n",
            "        [-0.0130,  0.0240,  0.0254,  ..., -0.0072, -0.0142,  0.0142],\n",
            "        [-0.0112, -0.0019, -0.0249,  ...,  0.0269, -0.0133,  0.0106]],\n",
            "       device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-0.0052,  0.0101, -0.0055,  0.0226,  0.0274,  0.0052, -0.0109, -0.0196,\n",
            "         0.0171,  0.0173, -0.0175,  0.0098, -0.0054, -0.0236, -0.0054,  0.0168,\n",
            "        -0.0261, -0.0215, -0.0223, -0.0211, -0.0213,  0.0046,  0.0240,  0.0072,\n",
            "        -0.0243, -0.0064,  0.0078,  0.0162], device='cuda:0',\n",
            "       requires_grad=True)]\n",
            "torch.Size([28, 1280])\n",
            "torch.Size([28])\n",
            "torch.Size([4, 1280])\n",
            "torch.Size([4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72xHKgN4RLah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "b7bb0805-e3a9-4626-859a-75ca803ecdd7"
      },
      "source": [
        "trainer.train(num_epochs=10,batch_report=50) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting at epoch 0\n",
            "==================================================\n",
            "EPOCH  0\n",
            "\n",
            "Last 10 Batch Avg Metrics, Batch 50/722\n",
            "Total Loss: 78944.727\n",
            "Classification Acc: 0.100\n",
            "BBox RMSE: 624.127\n",
            "Avg Bbox IoU: 0.056 \n",
            "\n",
            "Checkpoint created\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}