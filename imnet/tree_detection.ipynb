{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shubhomb/greenstand_data_analysis/blob/master/imnet/tree_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV4G09d8nrxd"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['treetracker-training-images/imnet/bounding_boxes',\n",
       " 'treetracker-training-images/imnet/original_images']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import os \n",
    "from PIL import Image\n",
    "import s3fs\n",
    "\n",
    "_TRAINING_SET = \"imnet\"\n",
    "_BUCKET = \"treetracker-training-images\"\n",
    "MODEL_SAVEPATH = \"/models/imnet/\"\n",
    "S3_PATH = 's3://%s/%s/'%(_BUCKET, _TRAINING_SET) # this is a root directory of S3 files to be used. Something like treetracker-training-images/[dataset_name]\n",
    "S3_FILESYSTEM = s3fs.S3FileSystem() # A global that will be used in dataset creation. Todo: Figure out a programatically better way to do this\n",
    "# To List 5 files in your accessible bucket\n",
    "fs.ls(S3_PATH)[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoKegeKl28VU"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36.0
    },
    "id": "MojfFLIl2-En",
    "outputId": "50e9c562-3011-4ffe-ca0a-4f7f2baba910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Torch Dataset and IMNet Loading\n",
    "import torch\n",
    "from xml.etree import ElementTree\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from  collections import OrderedDict\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "\n",
    "# Model development and training\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "# Filesystem and parallelization\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Utility \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Constants for parallelization\n",
    "_DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# from ImageNet\n",
    "tree_synsets = {\n",
    "    \"judas\": \"n12513613\",\n",
    "    \"palm\": \"n12582231\",\n",
    "    \"pine\": \"n11608250\",\n",
    "    \"china tree\": \"n12741792\",\n",
    "    \"fig\": \"n12401684\",\n",
    "    \"cabbage\": \"n12478768\",\n",
    "    \"cacao\": \"n12201580\",\n",
    "    \"kapok\": \"n12190410\",\n",
    "    \"iron\": \"n12317296\",\n",
    "    \"linden\": \"n12202936\",\n",
    "    \"pepper\": \"n12765115\",\n",
    "    \"rain\": \"n11759853\",\n",
    "    \"dita\": \"n11770256\",\n",
    "    \"alder\": \"n12284262\",\n",
    "    \"silk\": \"n11759404\",\n",
    "    \"coral\": \"n12527738\",\n",
    "    \"huisache\": \"n11757851\",\n",
    "    \"fringe\": \"n12302071\",\n",
    "    \"dogwood\": \"n12946849\",\n",
    "    \"cork\": \"n12713866\",\n",
    "    \"ginkgo\": \"n11664418\",\n",
    "    \"golden shower\": \"n12492106\",\n",
    "    \"balata\": \"n12774299\",\n",
    "    \"baobab\": \"n12189987\",\n",
    "    \"sorrel\": \"n12242409\",\n",
    "    \"Japanese pagoda\": \"n12570394\",\n",
    "    \"Kentucky coffee\": \"n12496427\",\n",
    "    \"Logwood\": \"n12496949\"\n",
    "}\n",
    "nontree_synsets = {\n",
    "   # Nontrees\n",
    "    \"garbage_bin\": \"n02747177\",\n",
    "    \"carion_fungus\": \"n13040303\",\n",
    "    \"basidiomycetous_fungus\": \"n13049953\",\n",
    "    \"jelly_fungus\": \"n13060190\",\n",
    "    \"desktop_computer\": \"n03180011\",\n",
    "    \"laptop_computer\": \"n03642806\",\n",
    "    \"cellphone\": \"n02992529\",\n",
    "    \"desk\": \"n03179701\",\n",
    "    \"station_wagon\": \"n02814533\",\n",
    "    \"pickup_truck\": \"n03930630\",\n",
    "    \"trailer_truck\": \"n04467665\"\n",
    "}\n",
    "synsets = {**tree_synsets, **nontree_synsets}\n",
    "print (\"Device:\" , _DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['treetracker-training-images/imnet/original_images/Japanese pagoda',\n",
       " 'treetracker-training-images/imnet/original_images/Kentucky coffee',\n",
       " 'treetracker-training-images/imnet/original_images/Logwood',\n",
       " 'treetracker-training-images/imnet/original_images/alder',\n",
       " 'treetracker-training-images/imnet/original_images/balata',\n",
       " 'treetracker-training-images/imnet/original_images/baobab',\n",
       " 'treetracker-training-images/imnet/original_images/basidiomycetous_fungus',\n",
       " 'treetracker-training-images/imnet/original_images/cabbage',\n",
       " 'treetracker-training-images/imnet/original_images/cacao',\n",
       " 'treetracker-training-images/imnet/original_images/carion_fungus',\n",
       " 'treetracker-training-images/imnet/original_images/cellphone',\n",
       " 'treetracker-training-images/imnet/original_images/china tree',\n",
       " 'treetracker-training-images/imnet/original_images/coral',\n",
       " 'treetracker-training-images/imnet/original_images/cork',\n",
       " 'treetracker-training-images/imnet/original_images/desktop_computer',\n",
       " 'treetracker-training-images/imnet/original_images/dita',\n",
       " 'treetracker-training-images/imnet/original_images/dogwood',\n",
       " 'treetracker-training-images/imnet/original_images/fig',\n",
       " 'treetracker-training-images/imnet/original_images/fringe',\n",
       " 'treetracker-training-images/imnet/original_images/garbage_bin',\n",
       " 'treetracker-training-images/imnet/original_images/ginkgo',\n",
       " 'treetracker-training-images/imnet/original_images/golden shower',\n",
       " 'treetracker-training-images/imnet/original_images/huisache',\n",
       " 'treetracker-training-images/imnet/original_images/iron',\n",
       " 'treetracker-training-images/imnet/original_images/jelly_fungus',\n",
       " 'treetracker-training-images/imnet/original_images/judas',\n",
       " 'treetracker-training-images/imnet/original_images/kapok',\n",
       " 'treetracker-training-images/imnet/original_images/laptop_computer',\n",
       " 'treetracker-training-images/imnet/original_images/linden',\n",
       " 'treetracker-training-images/imnet/original_images/palm',\n",
       " 'treetracker-training-images/imnet/original_images/pepper',\n",
       " 'treetracker-training-images/imnet/original_images/pickup_truck',\n",
       " 'treetracker-training-images/imnet/original_images/pine',\n",
       " 'treetracker-training-images/imnet/original_images/rain',\n",
       " 'treetracker-training-images/imnet/original_images/silk',\n",
       " 'treetracker-training-images/imnet/original_images/sorrel',\n",
       " 'treetracker-training-images/imnet/original_images/station_wagon',\n",
       " 'treetracker-training-images/imnet/original_images/trailer_truck']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(S3_FILESYSTEM.ls (S3_PATH + \"original_images/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wshbg0pg3IHX"
   },
   "source": [
    "\n",
    "## Dataset Creation\n",
    "Define datasets for ImageNet and Greenstand sources. Greenstand species classes are yet unlabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "LJ2MNziQ3B-B"
   },
   "outputs": [],
   "source": [
    "class ImnetDataset(data.Dataset):\n",
    "    \n",
    "    # initialise function of class\n",
    "    def __init__(self, dir, synsets, transforms=None, device=None, one_hot=False, nontrees=False):\n",
    "        # the data directories\n",
    "        self.img_dir = os.path.join(dir, \"original_images\")\n",
    "        self.bb_dir = os.path.join(dir, \"bounding_boxes\")\n",
    "        self.nontrees_present = nontrees\n",
    "        #synsets library to get the associated class\n",
    "        if not self.nontrees_present:  # only tree images\n",
    "          self.synsets = tree_synsets\n",
    "        else: # mix other things\n",
    "          self.synsets = synsets\n",
    "        self.rev_synsets = {y:x for x,y in zip(synsets.keys(), synsets.values())}\n",
    "        self.classes = list(self.synsets.keys())\n",
    "\n",
    "        self.one_hot = one_hot\n",
    "        self.imgs = []\n",
    "        self.file_stream = io.StringIO()\n",
    "\n",
    "        for i in self.classes:\n",
    "          temp_imgs = S3_FILESYSTEM.ls(os.path.join(self.img_dir, i))\n",
    "          for img_path in temp_imgs:\n",
    "            if not \"tar\" in img_path:\n",
    "              name = os.path.basename(img_path.split('.')[0])\n",
    "              self.imgs.append(name)\n",
    "\n",
    "        self.bb_dict = {}\n",
    "        for f, _, d in S3_FILESYSTEM.walk(self.bb_dir):\n",
    "          for file in d:\n",
    "            if os.path.splitext(file)[1] == \".xml\" and file.split(\"_\")[0] in tree_synsets.values():\n",
    "                with S3_FILESYSTEM.open(os.path.join(f, file)) as s3file:\n",
    "                  tree = ElementTree.parse(s3file)\n",
    "                  root = tree.getroot()\n",
    "                  obj = root.find(\"object\")\n",
    "                  b = obj.find(\"bndbox\")\n",
    "                  xmin = int(b.find(\"xmin\").text)\n",
    "                  ymin = int(b.find(\"ymin\").text)\n",
    "                  xmax = int(b.find(\"xmax\").text)\n",
    "                  ymax = int(b.find(\"ymax\").text)\n",
    "                  self.bb_dict[os.path.join(f, file)] =  (xmin, ymin, xmax, ymax)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.imgs[idx]\n",
    "        label = self.rev_synsets[name.split(\"_\")[0]]\n",
    "        # modify filters to determine if trees present\n",
    "        is_tree = 1.0\n",
    "        if self.nontrees_present:\n",
    "          if label in tree_synsets.keys():\n",
    "            is_tree = 1.0\n",
    "          else:\n",
    "            is_tree = 0.0\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, label, f\"{name}.JPEG\")\n",
    "        bb_path = os.path.join(self.bb_dir, label, \"Annotation\", name.split(\"_\")[0], f\"{name}.xml\")\n",
    "        with S3_FILESYSTEM.open(img_path) as f:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        if bb_path in self.bb_dict.keys():\n",
    "          xmin, ymin, xmax, ymax = self.bb_dict[bb_path]\n",
    "        else:\n",
    "          # the whole image is the bounding box label, as NoneType was causing collating issue. \n",
    "          xmin = 0\n",
    "          ymin = 0\n",
    "          xmax = img.size[0]\n",
    "          ymax = img.size[1]\n",
    "        boxes = torch.as_tensor([xmin, ymin, xmax, ymax], dtype=torch.float32)\n",
    "        if not is_tree:\n",
    "          boxes = torch.as_tensor([0, 0, 0, 0], dtype=torch.float32)  # 0 out nontree bounding boxes, don't want predictions for these\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "          img = self.transforms(img)\n",
    "\n",
    "        if self.one_hot: \n",
    "          image_id = torch.zeros(len(self.classes), dtype=torch.float32)\n",
    "          image_id [self.classes.index(label)] = 1.0\n",
    "        else:\n",
    "          image_id = torch.tensor([self.classes.index(label)])\n",
    "\n",
    "        targets = {}\n",
    "        targets[\"boxes\"] = boxes\n",
    "        targets[\"image_class\"] = image_id\n",
    "        targets[\"is_tree\"] = is_tree\n",
    "    \n",
    "        return img, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "QOicVbPgZrK1"
   },
   "outputs": [],
   "source": [
    "class GreenstandDataset(data.Dataset):\n",
    "  # We don't have labels for this yet...\n",
    "  def __init__(self, dir, device, transforms, bb_dir=None, one_hot=False):\n",
    "    self.img_dir = dir\n",
    "    self.imgs = []\n",
    "    self.bb_dir = bb_dir\n",
    "    self.transforms = transforms\n",
    "    self.classes = None # Change this when we define Greenstand class labels\n",
    "    self.one_hot = one_hot\n",
    "    self.device = device\n",
    "\n",
    "    for f, _, d in os.walk(test_path):\n",
    "      for fil in d:\n",
    "        if \"jpg\" in fil:\n",
    "          self.imgs.append(os.path.join(f, fil))\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = Image.open(self.img_dir[idx])\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms (img)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAGG9gAcEgdv"
   },
   "source": [
    "## MobileNet-v2\n",
    "See [the original paper](https://arxiv.org/pdf/1704.04861.pdf) for details. This was chosen first because Torchvision has pretrained weights and the net is quite low-latency, which may be useful for user-interface image selection. First go is to simply change the output layer to predict 4 coordinates for the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86.0,
     "referenced_widgets": [
      "9e1f88b442454b538d64c9cff7505411",
      "fb3c6a1d4edc4c78b1b2ad823838bbf1",
      "e7bee5115bcc4142b743981a0011bdf4",
      "05125a8cfbb1478ea40a0772830b4677",
      "d86ed8e90eda4e4a90d429232fc9e62a",
      "d217d5c345d840c58404121e1f5257e9",
      "04f548f0be6b461e9252fb102896eea0",
      "f62c4ea6649a45bf88564acba82add1a"
     ]
    },
    "id": "5g48vTx64Wdz",
    "outputId": "d2b7daf8-b973-4b11-bc5a-4fa33b8a7b64"
   },
   "outputs": [],
   "source": [
    "mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "# Preprocessing required for MobileNet v2\n",
    "mobilenet_preprocessing = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1JpU4M_iEY9"
   },
   "source": [
    "### Instantiate datasets, define loader processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36.0
    },
    "id": "kqgQQtXy3b0q",
    "outputId": "b0c080d1-8c02-4237-d9e4-67983ad34ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating datasets in  436.7641170024872  seconds \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_set = ImnetDataset(s3_path, synsets, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE, nontrees=True)\n",
    "# greenstand_test = GreenstandDataset(test_path, transforms=mobilenet_preprocessing, one_hot=False, device=_DEVICE)\n",
    "\n",
    "print (\"Finished creating datasets in \", time.time() - start, \" seconds \") # this should take <15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "kk6-8FA8Ab8v"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Helper functions \n",
    "def rmse(x, y):\n",
    "  '''\n",
    "  Root-mean squared error of two vectors of the same batch \n",
    "  '''\n",
    "  return torch.sqrt( (1/x.size()[0]) * torch.sum((x-y) **2))\n",
    "\n",
    "def iou(box_a, box_b):\n",
    "  # order is xmin, ymin, xmax, ymax \n",
    "  intersect_xmin = max(box_a[0], box_b[0])\n",
    "  intersect_ymin = max(box_a[1], box_b[1])\n",
    "  intersect_xmax = min(box_a[2], box_b[2])\n",
    "  intersect_ymax = min(box_a[3], box_b[3])\n",
    "  area_intersect = max(0, intersect_xmax - intersect_xmin) * max(0, intersect_ymax - intersect_ymin)\n",
    "\n",
    "  area_a = (box_a[3] - box_a[1]) * (box_a[2] - box_a[0])\n",
    "  area_b = (box_b[3] - box_b[1]) * (box_b[2] - box_b[0])\n",
    "  union = area_a + area_b - area_intersect\n",
    "  return area_intersect / union\n",
    "\n",
    "class Customized_MobileNet(nn.Module):\n",
    "  def __init__(self, pretrained_model):\n",
    "    super().__init__()\n",
    "    self.pretrained = pretrained_model\n",
    "    self.pretrained.classifier = nn.Identity()\n",
    "    for param in self.pretrained.parameters():\n",
    "      param.requires_grad = False\n",
    "    self._binary_classifier_layer()\n",
    "    self._regressor_layer()\n",
    "\n",
    "    \n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    The model is performing a regression on bounding boxes and a classifier \n",
    "    \"\"\"\n",
    "    return self.classifier(self.pretrained(x)), self.regressor(self.pretrained(x))\n",
    "\n",
    "  def _binary_classifier_layer(self):\n",
    "    \"\"\"\n",
    "    Initializes final classification layer for labeling genus, species, etc.\n",
    "    \"\"\"\n",
    "    self.classifier = nn.Sequential(\n",
    "                        nn.Dropout(0.2),\n",
    "                        nn.Linear(1280, 1), # 1280 is num_outputs of the last feature layer\n",
    "                      ) \n",
    "    for param in self.classifier.parameters():\n",
    "      param.requires_grad = True\n",
    "\n",
    "  def _regressor_layer(self):\n",
    "    \"\"\"\n",
    "    A bounding box output layer for predicting object location\n",
    "    This is currently designed to output exactly one bounding box\n",
    "    \"\"\"\n",
    "    self.regressor =  nn.Sequential(\n",
    "                        nn.Dropout(0.2), \n",
    "                        nn.Linear(1280, 4) # 1280 is num_outputs of the last feature layer\n",
    "                      )\n",
    "    for param in self.regressor.parameters():\n",
    "      param.requires_grad=True\n",
    "\n",
    "class ModelTrainer():\n",
    "  '''\n",
    "  An abstraction to help keep track of model parameters and run training. \n",
    "  '''\n",
    "  def __init__ (self, model, dataset, learning_rate, device, batch_size, model_savepath,\n",
    "                gamma=1e-4, train_split=0.8, pin_memory=False, n_workers=0,\n",
    "                alpha=0.5, beta=0.5\n",
    "                ):\n",
    "    \n",
    "    self.model = model # like Customized_MobileNet\n",
    "    self.model_savepath = os.path.join (model_savepath, \"checkpoint.pth.tar\")\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta\n",
    "    # Initialize device\n",
    "    self.device = device\n",
    "    if self.device == torch.device(\"cuda:0\"):\n",
    "      self.model.cuda()\n",
    "    \n",
    "\n",
    "    # Make validation split\n",
    "    self.trainsize = int(train_split * len(dataset))\n",
    "    self.valsize = len(dataset) - self.trainsize\n",
    "    train_dataset, valid_dataset = torch.utils.data.dataset.random_split(dataset, [self.trainsize, self.valsize])\n",
    "\n",
    "    # Define data loader for training and validation\n",
    "    self.batch_size = batch_size\n",
    "    self.data_loader  = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
    "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
    "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
    "              worker_init_fn=None)\n",
    "\n",
    "    self.val_data_loader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=True, sampler=None,\n",
    "              batch_sampler=None, num_workers=n_workers, collate_fn=None,\n",
    "              pin_memory=pin_memory, drop_last=False, timeout=0,\n",
    "              worker_init_fn=None)\n",
    "    \n",
    "    # Loss specifications and optimizer parameter setting\n",
    "    # This is defined here so that the underlying model can be changed (i.e. hidden layers) \n",
    "    cps = [param for param in self.model.classifier.parameters()]\n",
    "    rps = [param for param in self.model.regressor.parameters()]\n",
    "    self.optimizer = torch.optim.Adam(params=cps+rps, lr=learning_rate, weight_decay=gamma)\n",
    "    self.binary_classification_criterion = nn.BCEWithLogitsLoss()\n",
    "    self.regression_criterion = nn.MSELoss()\n",
    "\n",
    "    if os.path.exists(self.model_savepath):\n",
    "      print (\"Found saved model at savepath %s\" %(self.model_savepath))\n",
    "      checkpoint = torch.load(self.model_savepath)\n",
    "      self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      self.start_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "      self.start_epoch = 0\n",
    "\n",
    "\n",
    "  def describe_training(self):\n",
    "    print (self.trainsize, \" training examples\")\n",
    "    print (self.valsize, \" validation examples\")\n",
    "\n",
    "  def train(self, num_epochs, val_interval=1, batch_report=50, batch_lookback=10):\n",
    "    '''\n",
    "    Main function to train. \n",
    "    @param num_epochs(int): Number of epochs of training\n",
    "    @param val_interval(int): Interval epochs between validation metric\n",
    "    @param batch_report(int): Interval batches between training reports\n",
    "    @param batch_lookback(int): Number of batches to use for averaging metrics in printing\n",
    "    '''\n",
    "    num_tr_batches = np.ceil(self.trainsize/self.data_loader.batch_size)\n",
    "    num_val_batches = np.ceil(self.valsize/self.valsize)\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "    epoch_rmse = []\n",
    "    epoch_iou = []\n",
    "    val_epoch_loss = []\n",
    "    val_epoch_acc = []\n",
    "    val_epoch_rmse = []\n",
    "    val_epoch_iou = []\n",
    "    print (\"Starting at epoch %d\"%self.start_epoch)\n",
    "    for epoch in range(self.start_epoch, num_epochs):\n",
    "      epoch_start = time.time()\n",
    "      print (\"=\" * 50)\n",
    "      print (\"EPOCH \", epoch)\n",
    "      batch_count = 0\n",
    "      batch_loss = []\n",
    "      batch_acc = []\n",
    "      batch_rmse = []\n",
    "      batch_iou = []\n",
    "      for batchx, batchy in self.data_loader:\n",
    "          batch_count += 1\n",
    "          # Device designation\n",
    "          if self.device == torch.device(\"cuda:0\"):\n",
    "            batchx = batchx.cuda(non_blocking=True)\n",
    "            batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
    "            batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
    "            batchy[\"is_tree\"] = batchy[\"is_tree\"].cuda(non_blocking=True)\n",
    "          class_labels = batchy[\"image_class\"]\n",
    "          box_labels = batchy[\"boxes\"]\n",
    "          is_tree_labels = batchy[\"is_tree\"]\n",
    "\n",
    "          # Forward pass\n",
    "          is_tree_preds, box_preds = self.model.forward(batchx)\n",
    "          loss = self._loss_specification(is_tree_labels, is_tree_preds, box_labels, box_preds)\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "          # Metrics \n",
    "          box_rmse = rmse(box_preds, box_labels)\n",
    "          avg_box_iou = torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32))\n",
    "          binary_correct = (torch.round(is_tree_preds) == is_tree_labels.squeeze()).sum()\n",
    "          acc = binary_correct/float(batchx.shape[0])\n",
    "          batch_iou.append(avg_box_iou)\n",
    "          batch_rmse.append(box_rmse)\n",
    "          batch_acc.append(acc)\n",
    "          batch_loss.append(loss.data)\n",
    "\n",
    "          if batch_count % batch_report == 0 or batch_count == num_tr_batches:\n",
    "            print (\"\\nLast %d Batch Avg Metrics, Batch %d/%d\" %(batch_lookback, batch_count, num_tr_batches))\n",
    "            print (\"Total Loss: {:.3f}\".format(torch.mean(torch.as_tensor(batch_loss[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"Classification Acc: {:.3f}\".format(torch.mean(torch.as_tensor(batch_acc[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"BBox RMSE: {:.3f}\".format(torch.mean(torch.as_tensor(batch_rmse[-batch_lookback:], dtype=torch.float32))))\n",
    "            print (\"Avg Bbox IoU: {:.3f} \\n\".format(torch.mean(torch.as_tensor(batch_iou[-batch_lookback:], dtype=torch.float32))))\n",
    "            torch.save({\n",
    "                          'epoch': epoch + 1,\n",
    "                          'model_state_dict': self.model.state_dict(),\n",
    "                          'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                          }, \n",
    "                       self.model_savepath)\n",
    "            print (\"Checkpoint created\")\n",
    "      \n",
    "      if epoch % val_interval == 0:\n",
    "          print (\"VALIDATION EPOCH \", epoch)\n",
    "          batch_count = 0\n",
    "          \n",
    "          self.model.eval()\n",
    "          with torch.no_grad():\n",
    "            rmses = []\n",
    "            ious = []\n",
    "            losses = []\n",
    "            class_accs = []\n",
    "\n",
    "            for batchx, batchy in self.val_data_loader:\n",
    "                batch_count += 1\n",
    "                 # Device designation\n",
    "                if self.device == torch.device(\"cuda:0\"):\n",
    "                  batchx = batchx.cuda(non_blocking=True)\n",
    "                  batchy[\"boxes\"] = batchy[\"boxes\"].cuda(non_blocking=True)\n",
    "                  batchy[\"image_class\"] = batchy[\"image_class\"].cuda(non_blocking=True)\n",
    "                  batchy[\"is_tree\"] = batchy[\"is_tree\"].cuda(non_blocking=True)\n",
    "                class_labels = batchy[\"image_class\"]\n",
    "                box_labels = batchy[\"boxes\"]\n",
    "                is_tree_labels = batchy[\"is_tree\"]\n",
    "                is_tree_preds, box_preds = self.model.forward(batchx)\n",
    "                losses.append(self._loss_specification(is_tree_labels, is_tree_preds, box_labels, box_preds).data)\n",
    "                class_accs.append(float((torch.round(is_tree_preds) == is_tree_labels.squeeze()).sum())/self.val_data_loader.batch_size)\n",
    "                ious.append(torch.mean(torch.as_tensor([iou(box_labels[i, :], box_preds[i, :]) for i in range(box_labels.size()[0])], dtype=torch.float32)))\n",
    "                rmses.append(rmse(box_preds, box_labels))\n",
    "\n",
    "              \n",
    "            losses = torch.mean(torch.as_tensor(losses, dtype=torch.float32))\n",
    "            class_accs = torch.mean(torch.as_tensor(class_accs, dtype=torch.float32))\n",
    "            box_rmse = torch.mean(torch.as_tensor(rmses, dtype=torch.float32))\n",
    "            avg_box_iou = torch.mean(torch.as_tensor(ious, dtype=torch.float32))\n",
    "            val_epoch_loss.append(losses)\n",
    "            val_epoch_acc.append(class_accs)\n",
    "            val_epoch_rmse.append(box_rmse)\n",
    "            val_epoch_iou.append(avg_box_iou)\n",
    "\n",
    "            # We can change this to be epoch wise or not averaged over all batches\n",
    "            print (\"Batch Average Val Loss: {:.3f}\".format(losses))\n",
    "            print (\"Batch Avg Val Classification Acc: {:.3f}\".format(class_accs))\n",
    "            print (\"Batch Avg Val BBox RMSE: {:.3f}\".format(box_rmse))\n",
    "            print (\"Batch Avg Avg Bbox IoU: {:.3f} \\n\".format(avg_box_iou))\n",
    "          self.model.train()\n",
    "      epoch_loss.append(torch.mean(torch.as_tensor(batch_loss, dtype=torch.float32)))\n",
    "      epoch_acc.append(torch.mean(torch.as_tensor(batch_acc, dtype=torch.float32)))\n",
    "      epoch_iou.append(torch.mean(torch.as_tensor(batch_iou, dtype=torch.float32)))\n",
    "      epoch_rmse.append(torch.mean(torch.as_tensor(batch_rmse, dtype=torch.float32)))\n",
    "  \n",
    "      print (\"Epoch \", epoch + 1, \" finished in \", time.time() - epoch_start)\n",
    "    tr_metric_dict = {\"Loss\": epoch_loss, \"Acc\": epoch_acc, \"IoU\": epoch_iou, \"RMSE\": epoch_rmse}\n",
    "    val_metric_dict = {\"Loss\": val_epoch_loss, \"Acc\": val_epoch_acc, \"IoU\": val_epoch_iou, \"RMSE\": val_epoch_rmse}\n",
    "    torch.save({\n",
    "              'epoch': epoch + 1,\n",
    "              'model_state_dict': self.model.state_dict(),\n",
    "              'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "              'tr_metric_dict': tr_metric_dict, \n",
    "              'val_metric_dict': val_metric_dict\n",
    "              }, \n",
    "            self.model_savepath)\n",
    "    print (\"Final checkpoint created. Model dict and metrics saved. \")\n",
    "    return self.model, tr_metric_dict, val_metric_dict\n",
    "\n",
    "\n",
    "  def _loss_specification(self, is_tree_labels, is_tree_preds, box_labels, box_preds):\n",
    "    binary_detection_error = self.binary_classification_criterion(is_tree_preds, is_tree_labels.unsqueeze(1)) # output, target\n",
    "    bounding_box_error = self.regression_criterion(box_preds, box_labels)\n",
    "    return self.alpha * binary_detection_error + self.beta * bounding_box_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73.0
    },
    "id": "U6TrQ9DDL6HD",
    "outputId": "a08f9a6a-9289-4f2f-af9e-6574010dcc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33506  training examples\n",
      "8377  validation examples\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "_N_WORKERS = 0\n",
    "_PIN_MEM = True\n",
    "\n",
    "mob_model = Customized_MobileNet(pretrained_model=mobilenet)\n",
    "trainer = ModelTrainer(mob_model, \n",
    "                       data_set, \n",
    "                       train_split=0.8,\n",
    "                       learning_rate=0.002,\n",
    "                       batch_size = 64,\n",
    "                       device=_DEVICE, \n",
    "                       pin_memory=_PIN_MEM,\n",
    "                       n_workers=_N_WORKERS,\n",
    "                       model_savepath=MODEL_SAVEPATH)\n",
    "print (trainer.describe_training())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl5ULX4vu4YB"
   },
   "source": [
    "### Check model structure and parameters\n",
    "Regressor and classifier (final layer) should require grad. Others should not. Optimizer should be set only to those regressor and classifier variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353.0
    },
    "id": "SdO1y3kTtfpM",
    "outputId": "ede72e29-e68c-4c91-bf85-728013fb0ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor params\n",
      "[Parameter containing:\n",
      "tensor([[-0.0160,  0.0272,  0.0103,  ..., -0.0189,  0.0067,  0.0057],\n",
      "        [-0.0251,  0.0065, -0.0128,  ..., -0.0158, -0.0041,  0.0130],\n",
      "        [ 0.0015, -0.0010, -0.0226,  ..., -0.0005, -0.0206, -0.0046],\n",
      "        [ 0.0180,  0.0131, -0.0086,  ..., -0.0070,  0.0132,  0.0133]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0163, -0.0053,  0.0005,  0.0251], requires_grad=True)]\n",
      "Classifier params\n",
      "[Parameter containing:\n",
      "tensor([[ 0.0099,  0.0152, -0.0199,  ...,  0.0008, -0.0062, -0.0047]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0057], requires_grad=True)]\n",
      "torch.Size([1, 1280])\n",
      "torch.Size([1])\n",
      "torch.Size([4, 1280])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print (\"Regressor params\")\n",
    "print ([p for p in mob_model.regressor.parameters()])\n",
    "\n",
    "print (\"Classifier params\")\n",
    "print ([p for p in mob_model.classifier.parameters()])\n",
    "\n",
    "# Should output just two layers (4 variables total)\n",
    "for param in mob_model.parameters():\n",
    "  if param.requires_grad:\n",
    "    print (param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409.0
    },
    "id": "72xHKgN4RLah",
    "outputId": "e23058b9-cd65-4c19-e65e-2cbe1913d8e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at epoch 0\n",
      "==================================================\n",
      "EPOCH  0\n"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=10, batch_report=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tree_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04f548f0be6b461e9252fb102896eea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "05125a8cfbb1478ea40a0772830b4677": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f62c4ea6649a45bf88564acba82add1a",
      "placeholder": "​",
      "style": "IPY_MODEL_04f548f0be6b461e9252fb102896eea0",
      "value": " 13.6M/13.6M [00:00&lt;00:00, 63.0MB/s]"
     }
    },
    "9e1f88b442454b538d64c9cff7505411": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e7bee5115bcc4142b743981a0011bdf4",
       "IPY_MODEL_05125a8cfbb1478ea40a0772830b4677"
      ],
      "layout": "IPY_MODEL_fb3c6a1d4edc4c78b1b2ad823838bbf1"
     }
    },
    "d217d5c345d840c58404121e1f5257e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d86ed8e90eda4e4a90d429232fc9e62a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e7bee5115bcc4142b743981a0011bdf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d217d5c345d840c58404121e1f5257e9",
      "max": 1.4212972E7,
      "min": 0.0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d86ed8e90eda4e4a90d429232fc9e62a",
      "value": 1.4212972E7
     }
    },
    "f62c4ea6649a45bf88564acba82add1a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb3c6a1d4edc4c78b1b2ad823838bbf1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
